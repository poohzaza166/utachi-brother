{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe269965191346a4808a64f94595f6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The component <class 'transformers.models.clip.feature_extraction_clip.CLIPFeatureExtractor'> of <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> cannot be loaded as it does not seem to have any of the loading methods defined in {'ModelMixin': ['save_pretrained', 'from_pretrained'], 'SchedulerMixin': ['save_config', 'from_config'], 'DiffusionPipeline': ['save_pretrained', 'from_pretrained'], 'OnnxRuntimeModel': ['save_pretrained', 'from_pretrained'], 'PreTrainedTokenizer': ['save_pretrained', 'from_pretrained'], 'PreTrainedTokenizerFast': ['save_pretrained', 'from_pretrained'], 'PreTrainedModel': ['save_pretrained', 'from_pretrained'], 'FeatureExtractionMixin': ['save_pretrained', 'from_pretrained']}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     Image(image)\n\u001b[1;32m     49\u001b[0m     \u001b[39m# image.save(\"render.png\")\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m gen_waifu(\u001b[39m\"\u001b[39;49m\u001b[39mwhite hair witch\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m, in \u001b[0;36mgen_waifu\u001b[0;34m(query, width, height, num_inference_steps, seed, guidance_scale)\u001b[0m\n\u001b[1;32m     14\u001b[0m model_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhakurei/waifu-diffusion\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m pipe \u001b[39m=\u001b[39m StableDiffusionPipeline\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     18\u001b[0m     model_id,\n\u001b[1;32m     19\u001b[0m     \u001b[39m# torch_dtype=torch.,\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m     provider\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDmlExecutionProvider\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[1;32m     21\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat16,\n\u001b[1;32m     22\u001b[0m     revision\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfp16\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m     scheduler\u001b[39m=\u001b[39;49mDDIMScheduler(\n\u001b[1;32m     24\u001b[0m         beta_start\u001b[39m=\u001b[39;49m\u001b[39m0.00085\u001b[39;49m,\n\u001b[1;32m     25\u001b[0m         beta_end\u001b[39m=\u001b[39;49m\u001b[39m0.012\u001b[39;49m,\n\u001b[1;32m     26\u001b[0m         beta_schedule\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mscaled_linear\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     27\u001b[0m         clip_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     28\u001b[0m         set_alpha_to_one\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     \u001b[39m# width=width,\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m     \u001b[39m# height=height,\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m     \u001b[39m# num_inference_steps=num_inference_steps,\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m     \u001b[39m# guidance_scale=guidance_scale,\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m     \n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     36\u001b[0m pipe\u001b[39m.\u001b[39msafety_checker \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m images, clip_input: (images, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m pipe \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/utachi_brother-Oj3lSOlW/lib/python3.10/site-packages/diffusers/pipeline_utils.py:494\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[39mif\u001b[39;00m none_module\u001b[39m.\u001b[39mstartswith(DUMMY_MODULES_FOLDER) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdummy\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m none_module:\n\u001b[1;32m    491\u001b[0m         \u001b[39m# call class_obj for nice error message of missing requirements\u001b[39;00m\n\u001b[1;32m    492\u001b[0m         class_obj()\n\u001b[0;32m--> 494\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    495\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe component \u001b[39m\u001b[39m{\u001b[39;00mclass_obj\u001b[39m}\u001b[39;00m\u001b[39m of \u001b[39m\u001b[39m{\u001b[39;00mpipeline_class\u001b[39m}\u001b[39;00m\u001b[39m cannot be loaded as it does not seem to have\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    496\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m any of the loading methods defined in \u001b[39m\u001b[39m{\u001b[39;00mALL_IMPORTABLE_CLASSES\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    497\u001b[0m     )\n\u001b[1;32m    499\u001b[0m load_method \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(class_obj, load_method_name)\n\u001b[1;32m    500\u001b[0m loading_kwargs \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: The component <class 'transformers.models.clip.feature_extraction_clip.CLIPFeatureExtractor'> of <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> cannot be loaded as it does not seem to have any of the loading methods defined in {'ModelMixin': ['save_pretrained', 'from_pretrained'], 'SchedulerMixin': ['save_config', 'from_config'], 'DiffusionPipeline': ['save_pretrained', 'from_pretrained'], 'OnnxRuntimeModel': ['save_pretrained', 'from_pretrained'], 'PreTrainedTokenizer': ['save_pretrained', 'from_pretrained'], 'PreTrainedTokenizerFast': ['save_pretrained', 'from_pretrained'], 'PreTrainedModel': ['save_pretrained', 'from_pretrained'], 'FeatureExtractionMixin': ['save_pretrained', 'from_pretrained']}."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython.display import Image\n",
    "\n",
    "from torch import autocast\n",
    "import os\n",
    "os.environ['HSA_OVERRIDE_GFX_VERSION'] = \"10.3.0\"\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler , StableDiffusionOnnxPipeline\n",
    "#1366Ã—768\n",
    "def gen_waifu(query, width= 512, height= 512, num_inference_steps: int = 6, seed: int = 69420, guidance_scale: int = 2 ):\n",
    "    # width = 1024\n",
    "    # height = 1024\n",
    "    # num_inference_steps = 100\n",
    "    # seed = 69420\n",
    "    model_id = \"hakurei/waifu-diffusion\"\n",
    "    device = \"cuda\"\n",
    "\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        # torch_dtype=torch.,\n",
    "        provider=\"DmlExecutionProvider\", \n",
    "        torch_dtype=torch.float16,\n",
    "        revision=\"fp16\",\n",
    "        scheduler=DDIMScheduler(\n",
    "            beta_start=0.00085,\n",
    "            beta_end=0.012,\n",
    "            beta_schedule=\"scaled_linear\",\n",
    "            clip_sample=False,\n",
    "            set_alpha_to_one=False,\n",
    "        )\n",
    "        # width=width,\n",
    "        # height=height,\n",
    "        # num_inference_steps=num_inference_steps,\n",
    "        # guidance_scale=guidance_scale,\n",
    "        \n",
    "    )\n",
    "    pipe.safety_checker = lambda images, clip_input: (images, False)\n",
    "\n",
    "    pipe = pipe.to(device)\n",
    "\n",
    "    prompt = query\n",
    "    with autocast(\"cuda\"):\n",
    "        image = pipe(prompt, guidance_scale=guidance_scale,\n",
    "        width=width, height=height, \n",
    "        seed = seed,\n",
    "        num_inference_steps=num_inference_steps,)[\"sample\"][0]\n",
    "\n",
    "    # return image\n",
    "    Image(image)\n",
    "    # image.save(\"render.png\")\n",
    "\n",
    "gen_waifu(\"white hair witch\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utachi_brother-Oj3lSOlW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc34255581c6fa140fd915aaa86e924849aa2ce99ac1f3bc4bcc85e2052d303f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
